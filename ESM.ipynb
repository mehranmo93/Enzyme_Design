{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####The problem is designing de novo alpha-amylase enzyme to improve activity.\n",
        "\n",
        "I was provided with a dataset of alpha-amylase variants and their corresponding activity (sequencesID.csv)\n",
        "\n",
        "I obtained the per-residue representation of each sequence using ESM, added Gaussian noise to the non-conserved positions, and then mapped the modified representation back to sequence space. This approach is inspired by the [PePerCLIP](https://www.biorxiv.org/content/10.1101/2023.06.26.546591v2) model, which is used for peptide design."
      ],
      "metadata": {
        "id": "Em1JELoKbOYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVpNcURCa8CX"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install fair-esm\n",
        "!sudo apt-get install clustalo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding non-conserved positions\n",
        "So, we first need to find the non-conserved positions in the sequences, then add noise to the representations of those amino acids, and finally bring them back to the sequence space. Here, I want to identify the non-conserved positions."
      ],
      "metadata": {
        "id": "5KsooiFJc9pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Align import MultipleSeqAlignment\n",
        "from Bio.Align import AlignInfo\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import AlignIO\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/filtered_sequences.csv'\n",
        "sequences_df = pd.read_csv(file_path)\n",
        "\n",
        "sequences = sequences_df['mutated_sequence'].dropna().tolist()\n",
        "\n",
        "seq_records = [SeqRecord(Seq(seq), id=str(i)) for i, seq in enumerate(sequences)]\n",
        "\n",
        "# Perform multiple sequence alignment\n",
        "from Bio.Align.Applications import ClustalOmegaCommandline\n",
        "with open(\"input_sequences.fasta\", \"w\") as fasta_file:\n",
        "    for record in seq_records:\n",
        "        fasta_file.write(f\">{record.id}\\n{record.seq}\\n\")\n",
        "\n",
        "clustalomega_cline = ClustalOmegaCommandline(infile=\"input_sequences.fasta\", outfile=\"aligned.fasta\", verbose=True, auto=True)\n",
        "clustalomega_cline()\n",
        "\n",
        "alignment = AlignIO.read(\"aligned.fasta\", \"fasta\")\n",
        "\n",
        "# Identify non-conserved positions\n",
        "non_conserved_positions = []\n",
        "for i in range(len(alignment[0])):\n",
        "    column = [record.seq[i] for record in alignment]\n",
        "    if len(set(column)) > 1:  # Check if there's more than one amino acid type at this position\n",
        "        non_conserved_positions.append(i)\n",
        "\n",
        "print(\"Non-conserved positions:\", non_conserved_positions)\n"
      ],
      "metadata": {
        "id": "DhftsS4mdAHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying ESM\n",
        "\n",
        "In this block, I applied ESM to obtain the per-residue embedding of the sequence. The embedding dimension is (n * l), where n is the hidden dimension of ESM, and l is the length of the sequence. I then added Gaussian noise to the unconserved residues and mapped them back to sequence space. Using this approach, I can generate 500 different sequences. I used the first sequence, which had the highest activity, as the reference sequence."
      ],
      "metadata": {
        "id": "EktQoJ-ZdCZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "model_name = 'esm2_t6_8M_UR50D'  # Smaller model for demonstration\n",
        "model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "sequence = \"ATAPSIKSGTILHAWNWSFNTLKHNMKDIHDAGYTAIQTSPIMQVKEGNQGDKSMSNWYWLYQPTSYQIGNRYLGTEQEFKEMCAAAEEYGIKVIVDAVLNHTTSDYAAISNEVKSIPNWTHGNTPIKNWSDRWDVTQHSLLGLYDWNTQNTQVQSYLKRFLDRALNDGADGFRFDAAKHIELPDDGSYGSQFWPNITNTSAEFQYGEILQDSVSRDAAYANYMDVTASNYGHSIRSALKNRNLGVSNISHYAIDVSADKLVTWVESHDTYANDDEESTWMSDDDIRLGWAVIASRSGSTPLFFSRPEGGGNGVRFPGKSQIGDRGSALFEDQAITAVNRFHNVMAGQPEELSNPNGNNQIFMNQRGSHGVVLANAGSSSVSINTATKLPDGRYDNKAGAGSFQVNDGKLTGTINARSVAVLYAD\"  # Replace with your sequence\n",
        "sequence = sequence.upper()\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "data = [(\"protein1\", sequence)]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Obtain the model's representations for the sequence\n",
        "    results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
        "    token_embeddings = results[\"representations\"][model.num_layers]\n",
        "\n",
        "token_embeddings = token_embeddings[0, 1:-1]  # Shape: (L, D)\n",
        "\n",
        "# 6. Generate N sequences with noise added to specific positions\n",
        "N = 2000  # Number of sequences to generate\n",
        "k = 0.25  # Scaling factor for noise, set it small to avoid large changes\n",
        "\n",
        "L, D = token_embeddings.shape\n",
        "\n",
        "# Expand token embeddings to shape (N, L, D)\n",
        "token_embeddings_expanded = token_embeddings.unsqueeze(0).expand(N, L, D)\n",
        "\n",
        "noise = torch.zeros((N, L, D), device=device)\n",
        "\n",
        "# Add noise only to specific positions\n",
        "for pos in non_conserved_positions:\n",
        "    noise[:, pos, :] = torch.randn((N, D), device=device)\n",
        "\n",
        "perturbed_embeddings = token_embeddings_expanded + k * noise\n",
        "\n",
        "# Map perturbed embeddings back to amino acid probabilities\n",
        "final_layer = model.lm_head  # Linear layer: (embedding_dim) -> (alphabet_size)\n",
        "\n",
        "logits = final_layer(perturbed_embeddings)  # Shape: (N, L, alphabet_size)\n",
        "\n",
        "probabilities = torch.softmax(logits, dim=-1)  # Shape: (N, L, alphabet_size)\n",
        "\n",
        "predicted_tokens = torch.argmax(probabilities, dim=-1)  # Shape: (N, L)\n",
        "\n",
        "# Convert token indices back to amino acids\n",
        "tokens = alphabet.tok_to_idx\n",
        "idx_to_token = {idx: tok for tok, idx in tokens.items()}\n",
        "\n",
        "amino_acids = set(alphabet.standard_toks)\n",
        "original_first_amino_acid = sequence[0]\n",
        "\n",
        "sequences = []\n",
        "for i in range(N):\n",
        "    seq_tokens = predicted_tokens[i]  # Shape: (L,)\n",
        "    new_sequence = ''.join([idx_to_token[int(token)] for token in seq_tokens])\n",
        "    new_sequence = ''.join([aa if aa in amino_acids else '' for aa in new_sequence])\n",
        "\n",
        "    new_sequence = original_first_amino_acid + new_sequence[1:]\n",
        "\n",
        "    sequences.append(new_sequence)\n",
        "\n",
        "sequences_df = pd.DataFrame(sequences, columns=[\"Generated Sequence\"])\n",
        "sequences_df = sequences_df.drop_duplicates(subset=['Generated Sequence'])\n",
        "sequences_df.to_csv('/content/drive/My Drive/esm_generated_seq.csv', index=False)\n",
        "print (f'number of sequences generqted is', sequences_df.shape[0])\n",
        "\n",
        "print(\"Generated sequences saved to: /content/drive/My Drive/esm_generated_seq.csv\")\n"
      ],
      "metadata": {
        "id": "cRdusB7CeT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if generated sequences Are Nature-Like\n",
        "It is not necessary to run this block, as almost all of the sequences are nature-like, and the process is both time-consuming and computationally expensive. I only used it the first time to ensure accuracy when I initially generated the sequences."
      ],
      "metadata": {
        "id": "r4SB39MDeWs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "import time\n",
        "\n",
        "# Function to BLAST a protein sequence\n",
        "def blast_sequence(sequence):\n",
        "\n",
        "    result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", sequence)  # 'nr' is the non-redundant protein sequence database\n",
        "\n",
        "    # Parse the BLAST output\n",
        "    blast_records = NCBIXML.read(result_handle)\n",
        "\n",
        "    return blast_records\n",
        "\n",
        "# Function to check if the sequence is similar to any known protein\n",
        "def check_nature_like(blast_record):\n",
        "    for alignment in blast_record.alignments:\n",
        "        for hsp in alignment.hsps:\n",
        "            if hsp.expect < 0.01:  # E-value threshold\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "csv_file = '/content/generated_sequences.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "sequences = df.iloc[:, 0].tolist()  t\n",
        "\n",
        "# Check if sequences are nature-like with indexing\n",
        "nature_like_sequences = []\n",
        "for index, seq in enumerate(sequences, start=1):\n",
        "    print(f\"Blasting sequence number {index}...\")\n",
        "    blast_result = blast_sequence(seq)\n",
        "    is_nature_like = check_nature_like(blast_result)\n",
        "\n",
        "    if is_nature_like:\n",
        "        print(f\"Sequence number {index} is nature-like.\")\n",
        "        nature_like_sequences.append(seq)\n",
        "    else:\n",
        "        print(f\"Sequence number {index} is not nature-like.\")\n",
        "\n",
        "    time.sleep(5)  # Delay to avoid overwhelming NCBI's servers\n",
        "\n",
        "print(f\"Total nature-like sequences: {len(nature_like_sequences)}\")\n"
      ],
      "metadata": {
        "id": "ub1DoncpeYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, sequences were generated and need to be scored. I used PCS for dimensionality reduction, followed by Gaussian Process Regression with labeled data, but the results werenâ€™t great. Maybe it works only for peptide which is far smaller than my enzyme. For details on the method, you can check the Enzyme_Design_Final.ipynb notebook."
      ],
      "metadata": {
        "id": "DyNLJRnLebZ7"
      }
    }
  ]
}