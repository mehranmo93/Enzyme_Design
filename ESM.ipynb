{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####The problem is designing de novo alpha-amylase enzyme to improve activity.\n",
        "\n",
        "I was provided with a dataset of alpha-amylase variants and their corresponding activity (sequencesID.csv)\n",
        "\n",
        "I obtained the per-residue representation of each sequence using ESM, added Gaussian noise to the non-conserved positions, and then mapped the modified representation back to sequence space. This approach is inspired by the [PePerCLIP](https://www.biorxiv.org/content/10.1101/2023.06.26.546591v2) model, which is used for peptide design."
      ],
      "metadata": {
        "id": "Em1JELoKbOYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVpNcURCa8CX"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install fair-esm\n",
        "!sudo apt-get install clustalo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding non-conserved positions\n",
        "So, we first need to find the non-conserved positions in the sequences, then add noise to the representations of those amino acids, and finally bring them back to the sequence space. Here, I want to identify the non-conserved positions."
      ],
      "metadata": {
        "id": "5KsooiFJc9pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Align import MultipleSeqAlignment\n",
        "from Bio.Align import AlignInfo\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import AlignIO\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/filtered_sequences.csv'\n",
        "sequences_df = pd.read_csv(file_path)\n",
        "\n",
        "sequences = sequences_df['mutated_sequence'].dropna().tolist()\n",
        "\n",
        "seq_records = [SeqRecord(Seq(seq), id=str(i)) for i, seq in enumerate(sequences)]\n",
        "\n",
        "# Perform multiple sequence alignment\n",
        "from Bio.Align.Applications import ClustalOmegaCommandline\n",
        "with open(\"input_sequences.fasta\", \"w\") as fasta_file:\n",
        "    for record in seq_records:\n",
        "        fasta_file.write(f\">{record.id}\\n{record.seq}\\n\")\n",
        "\n",
        "clustalomega_cline = ClustalOmegaCommandline(infile=\"input_sequences.fasta\", outfile=\"aligned.fasta\", verbose=True, auto=True)\n",
        "clustalomega_cline()\n",
        "\n",
        "alignment = AlignIO.read(\"aligned.fasta\", \"fasta\")\n",
        "\n",
        "# Identify non-conserved positions\n",
        "non_conserved_positions = []\n",
        "for i in range(len(alignment[0])):\n",
        "    column = [record.seq[i] for record in alignment]\n",
        "    if len(set(column)) > 1:  # Check if there's more than one amino acid type at this position\n",
        "        non_conserved_positions.append(i)\n",
        "\n",
        "print(\"Non-conserved positions:\", non_conserved_positions)\n"
      ],
      "metadata": {
        "id": "DhftsS4mdAHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying ESM\n",
        "\n",
        "In this block, I applied ESM to obtain the per-residue embedding of the sequence. The embedding dimension is (n * l), where n is the hidden dimension of ESM, and l is the length of the sequence. I then added Gaussian noise to the unconserved residues and mapped them back to sequence space. Using this approach, I can generate 500 different sequences. I used the first sequence, which had the highest activity, as the reference sequence."
      ],
      "metadata": {
        "id": "EktQoJ-ZdCZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import esm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "model_name = 'esm2_t6_8M_UR50D'  # Smaller model for demonstration\n",
        "model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "sequence = \"ATAPSIKSGTILHAWNWSFNTLKHNMKDIHDAGYTAIQTSPIMQVKEGNQGDKSMSNWYWLYQPTSYQIGNRYLGTEQEFKEMCAAAEEYGIKVIVDAVLNHTTSDYAAISNEVKSIPNWTHGNTPIKNWSDRWDVTQHSLLGLYDWNTQNTQVQSYLKRFLDRALNDGADGFRFDAAKHIELPDDGSYGSQFWPNITNTSAEFQYGEILQDSVSRDAAYANYMDVTASNYGHSIRSALKNRNLGVSNISHYAIDVSADKLVTWVESHDTYANDDEESTWMSDDDIRLGWAVIASRSGSTPLFFSRPEGGGNGVRFPGKSQIGDRGSALFEDQAITAVNRFHNVMAGQPEELSNPNGNNQIFMNQRGSHGVVLANAGSSSVSINTATKLPDGRYDNKAGAGSFQVNDGKLTGTINARSVAVLYAD\"  # Replace with your sequence\n",
        "sequence = sequence.upper()\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "data = [(\"protein1\", sequence)]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Obtain the model's representations for the sequence\n",
        "    results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
        "    token_embeddings = results[\"representations\"][model.num_layers]\n",
        "\n",
        "token_embeddings = token_embeddings[0, 1:-1]  # Shape: (L, D)\n",
        "\n",
        "# 6. Generate N sequences with noise added to specific positions\n",
        "N = 2000  # Number of sequences to generate\n",
        "k = 0.25  # Scaling factor for noise, set it small to avoid large changes\n",
        "\n",
        "L, D = token_embeddings.shape\n",
        "\n",
        "# Expand token embeddings to shape (N, L, D)\n",
        "token_embeddings_expanded = token_embeddings.unsqueeze(0).expand(N, L, D)\n",
        "\n",
        "noise = torch.zeros((N, L, D), device=device)\n",
        "\n",
        "# Add noise only to specific positions\n",
        "for pos in non_conserved_positions:\n",
        "    noise[:, pos, :] = torch.randn((N, D), device=device)\n",
        "\n",
        "perturbed_embeddings = token_embeddings_expanded + k * noise\n",
        "\n",
        "# Map perturbed embeddings back to amino acid probabilities\n",
        "final_layer = model.lm_head  # Linear layer: (embedding_dim) -> (alphabet_size)\n",
        "\n",
        "logits = final_layer(perturbed_embeddings)  # Shape: (N, L, alphabet_size)\n",
        "\n",
        "probabilities = torch.softmax(logits, dim=-1)  # Shape: (N, L, alphabet_size)\n",
        "\n",
        "predicted_tokens = torch.argmax(probabilities, dim=-1)  # Shape: (N, L)\n",
        "\n",
        "# Convert token indices back to amino acids\n",
        "tokens = alphabet.tok_to_idx\n",
        "idx_to_token = {idx: tok for tok, idx in tokens.items()}\n",
        "\n",
        "amino_acids = set(alphabet.standard_toks)\n",
        "original_first_amino_acid = sequence[0]\n",
        "\n",
        "sequences = []\n",
        "for i in range(N):\n",
        "    seq_tokens = predicted_tokens[i]  # Shape: (L,)\n",
        "    new_sequence = ''.join([idx_to_token[int(token)] for token in seq_tokens])\n",
        "    new_sequence = ''.join([aa if aa in amino_acids else '' for aa in new_sequence])\n",
        "\n",
        "    new_sequence = original_first_amino_acid + new_sequence[1:]\n",
        "\n",
        "    sequences.append(new_sequence)\n",
        "\n",
        "sequences_df = pd.DataFrame(sequences, columns=[\"Generated Sequence\"])\n",
        "sequences_df = sequences_df.drop_duplicates(subset=['Generated Sequence'])\n",
        "sequences_df.to_csv('/content/drive/My Drive/esm_generated_seq.csv', index=False)\n",
        "print (f'number of sequences generqted is', sequences_df.shape[0])\n",
        "\n",
        "print(\"Generated sequences saved to: /content/drive/My Drive/esm_generated_seq.csv\")\n"
      ],
      "metadata": {
        "id": "cRdusB7CeT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if generated sequences Are Nature-Like\n",
        "It is not necessary to run this block, as almost all of the sequences are nature-like, and the process is both time-consuming and computationally expensive. I only used it the first time to ensure accuracy when I initially generated the sequences."
      ],
      "metadata": {
        "id": "r4SB39MDeWs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "import time\n",
        "\n",
        "# Function to BLAST a protein sequence\n",
        "def blast_sequence(sequence):\n",
        "\n",
        "    result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", sequence)  # 'nr' is the non-redundant protein sequence database\n",
        "\n",
        "    # Parse the BLAST output\n",
        "    blast_records = NCBIXML.read(result_handle)\n",
        "\n",
        "    return blast_records\n",
        "\n",
        "# Function to check if the sequence is similar to any known protein\n",
        "def check_nature_like(blast_record):\n",
        "    for alignment in blast_record.alignments:\n",
        "        for hsp in alignment.hsps:\n",
        "            if hsp.expect < 0.01:  # E-value threshold\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "csv_file = '/content/generated_sequences.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "sequences = df.iloc[:, 0].tolist()  t\n",
        "\n",
        "# Check if sequences are nature-like with indexing\n",
        "nature_like_sequences = []\n",
        "for index, seq in enumerate(sequences, start=1):\n",
        "    print(f\"Blasting sequence number {index}...\")\n",
        "    blast_result = blast_sequence(seq)\n",
        "    is_nature_like = check_nature_like(blast_result)\n",
        "\n",
        "    if is_nature_like:\n",
        "        print(f\"Sequence number {index} is nature-like.\")\n",
        "        nature_like_sequences.append(seq)\n",
        "    else:\n",
        "        print(f\"Sequence number {index} is not nature-like.\")\n",
        "\n",
        "    time.sleep(5)  # Delay to avoid overwhelming NCBI's servers\n",
        "\n",
        "print(f\"Total nature-like sequences: {len(nature_like_sequences)}\")\n"
      ],
      "metadata": {
        "id": "ub1DoncpeYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, sequences were generated and need to be scored. I used PCS for dimensionality reduction, followed by Gaussian Process Regression with labeled data, but the results weren’t great. Maybe it works only for peptide which is far smaller than my enzyme. For details on the method, you can check the Enzyme_Design_Final.ipynb notebook."
      ],
      "metadata": {
        "id": "DyNLJRnLebZ7"
      }
    }
  ]
}